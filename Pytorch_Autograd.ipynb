{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgBr9gnua8xaNe8sKeyHRC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Namesakenberg/Practical_deep_learning_using_pytorch/blob/main/Pytorch_Autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is the need of autograd ?\n",
        "\n",
        "-> **autograd is a tool in pytorch which helps us calculating the derivatives**.\n",
        "***autograd is largely used in training neural networks in pytorch .***\n",
        "\n",
        "Training of neural network has 4 steps :\n",
        "1) make prediction (forward propagation)\n",
        "2) calculate the loss\n",
        "3) backpropagation\n",
        "4) update the values of the weights and the bias terms\n",
        "\n",
        "## *backpropagation explained:*\n",
        "\n",
        "\n",
        "The process of backpropagation means calculating the gradient (partial differentaition of the loss function wrt parameters)\n",
        "once we find the gradient , its value can either be a positive or negative term.\n",
        "\n",
        "if it is positive : then **as the weight increses the loss increses , thus reduce the weight**\n",
        "\n",
        "and if it is negative : then **as the weight decreases the loss increases , thus increse the weights**\n",
        "\n",
        "thus the process of updating weights is done using the update rule.\n",
        "\n",
        "\n",
        "in this entire process , chain rule is used to calculated the partial derivative of the parameter wrt to the loss function\n",
        "\n",
        "**thus to calcualte all these derivatives on tensors (representing weights and biases autograd is used**)\n"
      ],
      "metadata": {
        "id": "XKrWfF7WB7oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# examples"
      ],
      "metadata": {
        "id": "NVWnriy-NMv0"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "tv_ZyzrENOWR"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## differentiating y=x^2"
      ],
      "metadata": {
        "id": "TtCJZv7SNqGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3.0 , requires_grad = True)\n",
        "\n",
        "# set the parameter requires grad as true , to get the derivative"
      ],
      "metadata": {
        "id": "HKz6Y0cCNQYu"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = x**2"
      ],
      "metadata": {
        "id": "PGYzuJY1Ngk5"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXW5K-xxNkrO",
        "outputId": "51e578cb-1165-43fd-bd19-17691be85661"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnnFWyjRNmJq",
        "outputId": "47f4ee8e-c5e3-4ac0-bfa1-643692cd8348"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(9., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# derivative\n",
        "\n",
        "y.backward()"
      ],
      "metadata": {
        "id": "TFE_QmxBOsgP"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hq23lrx3Ovtg",
        "outputId": "4838c9f3-74aa-4913-9ba1-c531887802e7"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# thus diff for y=3x^2 for =2 is 6"
      ],
      "metadata": {
        "id": "HFxjZLVWO0C3"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## example 2 :     \n",
        "\n",
        "q = p^2  , r = sin(p)"
      ],
      "metadata": {
        "id": "pKJ7iljAPQuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dz/dp = dz/dq * dq/dp"
      ],
      "metadata": {
        "id": "KAWa0BldRk2-"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = torch.tensor(3.0,requires_grad=True)"
      ],
      "metadata": {
        "id": "OidRciMtRwXd"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q= p**2"
      ],
      "metadata": {
        "id": "p2tAi0_uR4kb"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sin\n",
        "r = torch.sin(q)"
      ],
      "metadata": {
        "id": "dTIb15z3R-G0"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r.backward()"
      ],
      "metadata": {
        "id": "ncdhaS9WThnU"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIWv2-qrTj-v",
        "outputId": "5f98721c-28df-4a7d-eace-ac9b6547e0a7"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-5.4668)"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "internally pytorch makes a computation graph to store the sequence of operators in it , which would help it in applying the chain rule while finding the gradients"
      ],
      "metadata": {
        "id": "QeREkahITBuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# example 3 : A perceptron\n",
        "\n",
        "input = 6.7\n",
        "output = 0\n",
        "\n",
        "activation = sigmoid\n",
        "Loss function is binary cross entropy"
      ],
      "metadata": {
        "id": "cwQds1UuVBEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "x = torch.tensor(6.7 , requires_grad=True)\n",
        "y_test = torch.tensor([[0.0]])\n",
        "\n",
        "#print(type(y_test))\n",
        "\n",
        "w = torch.rand(1,1, requires_grad=True)\n",
        "b = torch.rand(1,1,requires_grad=True)\n",
        "z = torch.matmul(w,x.unsqueeze(0).unsqueeze(1)) + b\n",
        "\n",
        "y_pred = torch.sigmoid(z)\n",
        "\n",
        "#print(type(y_pred))\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "L = criterion(y_pred , y_test )\n",
        "\n",
        "L.backward()\n",
        "print(\"weight gradient\", w.grad)\n",
        "print(\"bias gradient \", b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF-l9BRpasQo",
        "outputId": "f74d0e55-fe21-4a70-bf87-45d166304fac"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weight gradient tensor([[6.5814]])\n",
            "bias gradient  tensor([[0.9823]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  .backward() -> partial derivative of\n",
        "#  .grad -> with respect to"
      ],
      "metadata": {
        "id": "1FZpphx0fvp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vector input tensor"
      ],
      "metadata": {
        "id": "LHfvUWDch_7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1.0,2.0,3.0] , requires_grad=True)\n",
        "y =(x**2).mean()    # muluti variable function based on all these values of x\n",
        "\n",
        "y.backward()\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIgjtSI5isCf",
        "outputId": "fe4f5d2a-4f50-4087-e39a-853631fdfa59"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6667, 1.3333, 2.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# clearning the gradient\n",
        "\n",
        "whenever we have do y.backward() and x.grad , we get the differentiated value of y wrt x , if we do this again ,the previous value is still stored , and thus the  result is the new value , which is not desireable\n"
      ],
      "metadata": {
        "id": "SlUUFayljruM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# thus to clear the gradient do\n",
        "x.grad.zero_()    # inplace operation to clear"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4crhaQtkJKf",
        "outputId": "528d2abd-ec86-40b1-b649-b8e7e8b7d2f5"
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disable gradient tracking"
      ],
      "metadata": {
        "id": "U2U6r7Wlkkdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient tracking is used when we are finding the derivatives while doing backward propagation"
      ],
      "metadata": {
        "id": "WBJSVRNikmcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# thus when we are done training the neural network , we should disable the gradient tracking"
      ],
      "metadata": {
        "id": "hzAWjniHk0gA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "\n",
        "a = torch.tensor(2.0 , requires_grad=True)\n",
        "b = a**2\n",
        "\n",
        "b.backward()\n",
        "a.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOnC1m5ElWA2",
        "outputId": "30e05d29-9289-4ffe-af1a-917215e0b9f8"
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "metadata": {},
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) disable gradient trackng\n",
        "a.requires_grad_(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg-9aZmFljtg",
        "outputId": "90148673-687a-4803-b36a-e428f4005e45"
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.)"
            ]
          },
          "metadata": {},
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) detach\n",
        "c = a.detach() # c will have final values of a  , but we cannot change its values"
      ],
      "metadata": {
        "id": "40RtBjuDlyFF"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) no grad\n",
        "with torch.no_grad():\n",
        "  b = a**2"
      ],
      "metadata": {
        "id": "y1Nt1rnwl5QY"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7roE-fccmTG1",
        "outputId": "04aca450-06ac-4382-97f4-f5eab3e2cce1"
      },
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "metadata": {},
          "execution_count": 234
        }
      ]
    }
  ]
}